{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a simple protein secondary structure predictor for predicting one of the three classes: coil, helix and beta sheet. Similar to [psipred](http://bioinf.cs.ucl.ac.uk/psipred/), we will use a window of amino acids around a residue and feed it into a neural network, but we will only use one neural network instead of two, and we will use a window of 21, i.e. 10 amino acids to either side of the one to be predicted. \n",
    "<br>\n",
    "<br> We will use a convolutional neural network with dense layers on top, and we will evaluate the quality of the predictions with the Q3 accuracy (basically accuracy across all residues) and the segment of overlap score. The segment of overlap score takes into account how much entire segments with the same secondary structure type overlap between the reference and the prediction, as opposed to the identity of individual amino acids.\n",
    "<br>\n",
    "<br> For training, validation and testing, we will use the data that was used to train the [jpred method](http://www.compbio.dundee.ac.uk/jpred4/about_RETR_JNetv231_details.shtml) as it is easily available and fulfills criteria such as a lack of structural/sequence homology between training and test set, which would lead to a biased evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load libraries, data and define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.loadtxt('train_names', dtype='str')\n",
    "test_ids = np.loadtxt('test_names', dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_items={}\n",
    "for i in train_ids:\n",
    "    with open('data/training/'+i+\".fasta\") as input:\n",
    "        seq = ''\n",
    "        lines = input.readlines()\n",
    "        for line in lines:\n",
    "            if line[0] != '>':\n",
    "                seq += line.strip()\n",
    "    with open('data/training/'+i+\".dssp\") as input:\n",
    "        ss = ''\n",
    "        lines = input.readlines()\n",
    "        for line in lines:\n",
    "            if line[0] != '>':\n",
    "                ss += line.strip()\n",
    "            \n",
    "    training_items[i] = {'seq':seq, 'ss':ss}\n",
    "    \n",
    "    \n",
    "test_items={}\n",
    "for i in test_ids:\n",
    "    with open('data/blind/'+i+\".fasta\") as input:\n",
    "        seq = ''\n",
    "        lines = input.readlines()\n",
    "        for line in lines:\n",
    "            if line[0] != '>':\n",
    "                seq += line.strip()\n",
    "    with open('data/blind/'+i+\".dssp\") as input:\n",
    "        ss = ''\n",
    "        lines = input.readlines()\n",
    "        for line in lines:\n",
    "            if line[0] != '>':\n",
    "                ss += line.strip()\n",
    "            \n",
    "    test_items[i] = {'seq':seq, 'ss':ss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_id_dict = {'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6, 'I': 7,\n",
    "              'K': 8, 'L': 9, 'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14, \n",
    "              'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19}\n",
    "\n",
    "pos_aa_dict = {j:i for i,j in aa_id_dict.items()}\n",
    "\n",
    "ss_id_dict = {'H':0, 'E':1, '-':2}\n",
    "\n",
    "pos_ss_dict = {j:i for i,j in ss_id_dict.items()}\n",
    "\n",
    "def aa_to_onehot(aa_str, aa_to_nr=aa_id_dict, mask=None):\n",
    "    \"\"\"\n",
    "    Onehot encode an amino acid string using a letter to number dictionary.\n",
    "    The mask (from proteinnet files) is used to remove residues missing atoms from the primary sequence.\n",
    "    \"\"\"\n",
    "    if mask!=None:\n",
    "        mask_ind = np.asarray([x=='+' for x in mask])*1\n",
    "        mask_ind = np.nonzero(mask_ind)\n",
    "        aa_str = \"\".join([aa_str[x] for x in mask_ind[0]]) # the mask indices are a list in a list\n",
    "    init_array = np.zeros( (len(aa_to_nr.keys()), len(aa_str)) )\n",
    "    for i,j in enumerate(aa_str):\n",
    "        init_array[aa_to_nr[j], i] = 1\n",
    "    return init_array\n",
    "\n",
    "def label_to_index(ss, id_dict):\n",
    "    labels = np.array([id_dict[i] for i in ss])\n",
    "    return(labels)\n",
    "\n",
    "def onehot_to_str(onehot_arr, map_dict=pos_aa_dict):\n",
    "    '''Helper function to recover aa sequence from onehot encoding\n",
    "        input must be aa*N numpy array'''\n",
    "    aas = []\n",
    "    N = onehot_arr.shape[1]\n",
    "    for i in range(N):\n",
    "        pos = np.where(onehot_arr[:, i]>0)[0]\n",
    "        aas.append(map_dict[int(pos)])\n",
    "    return \"\".join(aas)\n",
    "\n",
    "def filter_proteins(prot_id, seq, allowed_symbols):\n",
    "    allowed = True\n",
    "    for i in seq:\n",
    "        if i not in allowed_symbols:\n",
    "            allowed = False\n",
    "    return allowed\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll filter away proteins with missing/unidentified residues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds_filt = np.array([filter_proteins(i, training_items[i]['seq'], aa_id_dict.keys()) for i in train_ids])\n",
    "test_inds_filt = np.array([filter_proteins(i, test_items[i]['seq'], aa_id_dict.keys()) for i in test_ids])\n",
    "\n",
    "train_ids_filt = train_ids[train_inds_filt]\n",
    "test_ids_filt = test_ids[test_inds_filt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to pad the sequences on the ends and split them into smaller pieces of equal size for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_split_1h(vector, k, map_dict):\n",
    "    padding = np.zeros((len(map_dict.keys()), int((k-1)/2)))\n",
    "    vector_padded = np.concatenate((padding, aa_to_onehot(vector, map_dict), padding), axis=1)\n",
    "    sub_arrays = []\n",
    "    for i in range(int((k-1)/2), len(vector)+int((k-1)/2)):\n",
    "        sub_arr = vector_padded[:, i-int((k-1)/2):i+int((k-1)/2)+1]\n",
    "        sub_arrays.append(sub_arr)\n",
    "    sub_arrays = np.array(sub_arrays)\n",
    "    return torch.tensor(sub_arrays).float()\n",
    "\n",
    "def tensorize_ss(ss, map_dict=ss_id_dict, tensorize=True):\n",
    "    if tensorize:\n",
    "        tensor = torch.tensor(label_to_index(ss, map_dict))\n",
    "    else:\n",
    "        tensor = label_to_index(ss, map_dict)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pad_and_split_1h(list(training_items[train_ids_filt[0]]['ss']), 21, ss_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_window = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_ids_filt:\n",
    "    training_items[i]['seq_1h'] = pad_and_split_1h(training_items[i]['seq'], k_window, aa_id_dict)\n",
    "    \n",
    "for i in test_ids_filt:\n",
    "    test_items[i]['seq_1h'] = pad_and_split_1h(test_items[i]['seq'], k_window, aa_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(678)\n",
    "inds_perm = np.random.permutation(len(train_ids_filt))\n",
    "val_prots = train_ids_filt[inds_perm[0:int(np.floor(len(inds_perm)*0.2))]]\n",
    "train_prots = train_ids_filt[inds_perm[int(np.floor(len(inds_perm)*0.2)):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_tensor = torch.cat([training_items[i]['seq_1h'] for i in train_prots])\n",
    "train_ss_tensor = torch.cat([tensorize_ss(training_items[i]['ss']) for i in train_prots])\n",
    "\n",
    "val_seq_tensor = torch.cat([training_items[i]['seq_1h'] for i in val_prots])\n",
    "val_ss_tensor = torch.cat([tensorize_ss(training_items[i]['ss']) for i in val_prots])\n",
    "\n",
    "test_seq_tensor = torch.cat([test_items[i]['seq_1h'] for i in test_ids_filt])\n",
    "test_ss_tensor = torch.cat([tensorize_ss(test_items[i]['ss']) for i in test_ids_filt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([176746, 20, 21])\n",
      "torch.Size([41033, 20, 21])\n",
      "torch.Size([22734, 20, 21])\n"
     ]
    }
   ],
   "source": [
    "print(train_seq_tensor.shape)\n",
    "print(val_seq_tensor.shape)\n",
    "print(test_seq_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class proteindataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, seqs, ss):\n",
    "        self.sequences = seqs\n",
    "        self.ss = ss\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ss)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.sequences[idx,:, :], self.ss[idx]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = proteindataset(train_seq_tensor, train_ss_tensor)\n",
    "val_dataset = proteindataset(val_seq_tensor, val_ss_tensor)\n",
    "test_dataset = proteindataset(test_seq_tensor, test_ss_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=1000,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv0 = nn.Conv1d(20, 32, kernel_size=3)\n",
    "        self.conv0_bn = torch.nn.BatchNorm1d(32)\n",
    "        self.conv1 = nn.Conv1d(32, 64, kernel_size=2) # down\n",
    "        self.conv1_bn = torch.nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3) # down \n",
    "        self.conv2_bn = torch.nn.BatchNorm1d(128)\n",
    "        self.fc1 = nn.Linear(128, 8)\n",
    "        self.fc2 = nn.Linear(8, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv0_out = self.conv0_bn(self.pool(F.relu(self.conv0(x))))\n",
    "        conv1_out = self.conv1_bn(self.pool(F.relu(self.conv1(conv0_out))))\n",
    "        conv2_out = self.conv2_bn(self.pool(F.relu(self.conv2(conv1_out))))\n",
    "        conv2_out = conv2_out.view(-1, 128*1)\n",
    "        fc1_out = F.relu(self.fc1(conv2_out))\n",
    "        fc2_out = F.relu(self.fc2(fc1_out))\n",
    "\n",
    "        return fc2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 588] training loss: 1.0177401232395593, validation_loss: 0.9331093714350747\n",
      "epoch: 0, iteration: 1177] training loss: 0.9228928260163661, validation_loss: 0.8959935633909132\n",
      "new best validation loss, saving..\n",
      "epoch: 0, iteration: 1766] training loss: 0.8938826373153712, validation_loss: 0.8743328480493454\n",
      "new best validation loss, saving..\n",
      "epoch: 1, iteration: 588] training loss: 0.8659513940835445, validation_loss: 0.8577307235626948\n",
      "new best validation loss, saving..\n",
      "epoch: 1, iteration: 1177] training loss: 0.858353078972504, validation_loss: 0.8438456796464467\n",
      "new best validation loss, saving..\n",
      "epoch: 1, iteration: 1766] training loss: 0.8242927377819003, validation_loss: 0.8109213255700611\n",
      "new best validation loss, saving..\n",
      "epoch: 2, iteration: 588] training loss: 0.7880372604408976, validation_loss: 0.7905015434537616\n",
      "new best validation loss, saving..\n",
      "epoch: 2, iteration: 1177] training loss: 0.7698127213479707, validation_loss: 0.7805817027886708\n",
      "new best validation loss, saving..\n",
      "epoch: 2, iteration: 1766] training loss: 0.7548459155045463, validation_loss: 0.7727951137792495\n",
      "new best validation loss, saving..\n",
      "epoch: 3, iteration: 588] training loss: 0.7422817018521864, validation_loss: 0.7674505341620672\n",
      "new best validation loss, saving..\n",
      "epoch: 3, iteration: 1177] training loss: 0.7364999628633716, validation_loss: 0.7651381833212715\n",
      "new best validation loss, saving..\n",
      "epoch: 3, iteration: 1766] training loss: 0.7370260096770191, validation_loss: 0.7585734966255371\n",
      "new best validation loss, saving..\n",
      "epoch: 4, iteration: 588] training loss: 0.7211189015004752, validation_loss: 0.7580503367242358\n",
      "new best validation loss, saving..\n",
      "epoch: 4, iteration: 1177] training loss: 0.7211000981071809, validation_loss: 0.754817227522532\n",
      "new best validation loss, saving..\n",
      "epoch: 4, iteration: 1766] training loss: 0.718378077541021, validation_loss: 0.7532784853662763\n",
      "new best validation loss, saving..\n",
      "epoch: 5, iteration: 588] training loss: 0.7084848809930384, validation_loss: 0.7502401868502298\n",
      "new best validation loss, saving..\n",
      "epoch: 5, iteration: 1177] training loss: 0.7044992449935304, validation_loss: 0.7503112128802711\n",
      "epoch: 5, iteration: 1766] training loss: 0.7108863906868448, validation_loss: 0.7493143095856621\n",
      "new best validation loss, saving..\n",
      "epoch: 6, iteration: 588] training loss: 0.6959242077958604, validation_loss: 0.7487477262814839\n",
      "new best validation loss, saving..\n",
      "epoch: 6, iteration: 1177] training loss: 0.6971444138950322, validation_loss: 0.7490313180855341\n",
      "epoch: 6, iteration: 1766] training loss: 0.6995416269439996, validation_loss: 0.7473098933696746\n",
      "new best validation loss, saving..\n",
      "epoch: 7, iteration: 588] training loss: 0.6862572498353916, validation_loss: 0.7478427205766952\n",
      "epoch: 7, iteration: 1177] training loss: 0.691900649783162, validation_loss: 0.7491762368451982\n",
      "epoch: 7, iteration: 1766] training loss: 0.6878807044292347, validation_loss: 0.7494850045158751\n",
      "epoch: 8, iteration: 588] training loss: 0.6772663740026931, validation_loss: 0.7528383632500967\n",
      "epoch: 8, iteration: 1177] training loss: 0.6829955870813749, validation_loss: 0.7489747206370038\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "prints_per_epoch = 3\n",
    "\n",
    "verbose_k = np.floor(len(trainloader)/prints_per_epoch)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "iterations = []\n",
    "best_loss = None\n",
    "patience_val = 5\n",
    "patience_counter = patience_val\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        sequence, ss = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        predicted_angles = net(sequence)\n",
    "\n",
    "        loss = criterion(predicted_angles, ss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # adding to running loss, we will output this at every verbose_k\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (i+1) % verbose_k == 0:\n",
    "            if patience_counter < 1:\n",
    "                break\n",
    "            train_losses.append(running_loss/verbose_k)\n",
    "            true_iter = len(trainloader)*epoch + i\n",
    "            iterations.append(true_iter)\n",
    "            net.eval()\n",
    "            validation_loss = 0\n",
    "            for j in valloader:\n",
    "                pred_k = net(j[0])\n",
    "                loss_k = criterion(pred_k, j[1]).item()\n",
    "                validation_loss += loss_k/len(valloader)\n",
    "            val_losses.append(validation_loss)\n",
    "            net.train()\n",
    "            print('epoch: {}, iteration: {}] training loss: {}, validation_loss: {}'.format(\n",
    "                epoch, i, running_loss/verbose_k, validation_loss))\n",
    "\n",
    "            if best_loss == None:\n",
    "                best_loss = validation_loss\n",
    "            else:\n",
    "                if validation_loss <= min(val_losses):\n",
    "                    patience_counter = patience_val\n",
    "                    print('new best validation loss, saving..')\n",
    "                    best_loss = validation_loss\n",
    "                    torch.save(net.state_dict(), 'best_fcn_parameters.pt')\n",
    "                else:\n",
    "                    patience_counter -= 1\n",
    "            \n",
    "\n",
    "            running_loss = 0.0\n",
    "            \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('/Users/Deathvoodoo/big_folders_docs/ss_pred/best_fcn_parameters.pt'))\n",
    "\n",
    "net.eval()\n",
    "\n",
    "validation_preds = np.concatenate([np.apply_along_axis(np.argmax, 1, net(i[0]).detach().numpy()) for i in valloader])\n",
    "validation_reals = [tensorize_ss(training_items[i]['ss'], ss_id_dict, tensorize=False) for i in val_prots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3X90VeWd7/H31xgkoiUI1AsEMXZZCD8igUiZyyhaOoLaKnpHxdE76rRlabXttFNuQ1vBYWaWdHCmDHN1WsZla20r5XKVMtUurkWotaMt4VcUlMqvlgSrEU2qJU758b1/7J3DSciPs0/2SfY5+bzWOit7P/vZ+3yzk5xv9n728zzm7oiIiACc1tcBiIhIcigpiIhIipKCiIikKCmIiEiKkoKIiKQoKYiISEq3ScHMHjGzN83s5U62m5mtMLM9ZlZnZlPSth03s+3ha12cgYuISPwyuVL4DjCni+1XAheGr/nAv6Vta3H3yeHrmqyjFBGRXtFtUnD354C3u6hyLfBdD7wIlJrZiLgCFBGR3nN6DMcYBRxMW68Py14HBppZLXAMWOruazs6gJnNJ7jKYNCgQVPHjRsXQ1giIv3Hli1b3nL34T09ThxJoStj3L3BzC4AnjWzl9x9b/tK7r4SWAlQXV3ttbW1OQ5LRKSwmNlv4jhOHE8fNQCj09bLwjLcvfXrPmATUBXD+4mISI7EkRTWAX8ZPoU0HWh299fNbIiZnQFgZsOAGcCuGN5PRERypNvbR2b2OHAZMMzM6oHFQDGAu38TeBq4CtgDHAHuCHetAL5lZicIks9Sd1dSEBFJsG6Tgrvf3M12B+7uoPw/gUnZhyYiveXo0aPU19fz/vvv93Uo0o2BAwdSVlZGcXFxTo6f64ZmEckD9fX1nH322Zx//vmYWV+HI51wdw4fPkx9fT3l5eU5eQ8NcyEivP/++wwdOlQJIeHMjKFDh+b0ik5JQUQAlBDyRK5/TkoKIiKSoqQgIn2uqamJhx56KKt9r7rqKpqamjKuf9999/HAAw9k9V79gZKCiPS5rpLCsWPHutz36aefprS0NBdh9UtKCiIS2dptDcxY+izlNU8xY+mzrN3W0KPj1dTUsHfvXiZPnsyCBQvYtGkTl1xyCddccw3jx48HYO7cuUydOpUJEyawcuXK1L7nn38+b731FgcOHKCiooJPf/rTTJgwgSuuuIKWlpYu33f79u1Mnz6dyspKrrvuOt555x0AVqxYwfjx46msrGTevHkA/OxnP2Py5MlMnjyZqqoq3n333R59z4nl7ol6TZ061UWkd+3atSvjuk9urfdxX/uJj/nyj1OvcV/7iT+5tT7r99+/f79PmDAhtb5x40Y/88wzfd++famyw4cPu7v7kSNHfMKECf7WW2+5u/uYMWO8sbHR9+/f70VFRb5t2zZ3d7/hhhv8scceO+W9Fi9e7MuWLXN390mTJvmmTZvc3f3ee+/1z3/+8+7uPmLECH///ffd3f2dd95xd/ePf/zj/vzzz7u7+7vvvutHjx7N+vvtqY5+XkCtx/AZrCsFEYlk2frdtBw93qas5ehxlq3fHev7TJs2rc2z+CtWrOCiiy5i+vTpHDx4kNdee+2UfcrLy5k8eTIAU6dO5cCBA50ev7m5maamJmbOnAnAbbfdxnPPPQdAZWUlt9xyC9/73vc4/fSgO9eMGTP44he/yIoVK2hqakqVF5qCTApxX9qKyEmHmjq+JdNZebYGDRqUWt60aRM//elPeeGFF9ixYwdVVVUdPqt/xhlnpJaLioq6bY/ozFNPPcXdd9/N1q1bufjiizl27Bg1NTU8/PDDtLS0MGPGDF599dWsjp10BZcU1m5rYOETL9HQ1IIDDU0tLHziJSUGkZiMLC2JVJ6Js88+u8t79M3NzQwZMoQzzzyTV199lRdffDHr92o1ePBghgwZws9//nMAHnvsMWbOnMmJEyc4ePAgl19+OV//+tdpbm7mvffeY+/evUyaNIkvf/nLXHzxxUoK+aK3Lm1F+qsFs8dSUlzUpqykuIgFs8dmfcyhQ4cyY8YMJk6cyIIFC07ZPmfOHI4dO0ZFRQU1NTVMnz496/dK9+ijj7JgwQIqKyvZvn07ixYt4vjx49x6661MmjSJqqoqPve5z1FaWsry5cuZOHEilZWVFBcXc+WVV8YSQ9JY0D6RHD2dZKe85ik6+o4M2L/06qyPK1LIXnnlFSoqKjKuv3ZbA8vW7+ZQUwsjS0tYMHssc6tG5TBCSdfRz8vMtrh7dU+PXXAtJSNLS2jo4N5mTy5tRaStuVWjlAQKVMHdPsrFpa2ISH9RcFcKrf+96NJWRCS6gksKoEtbEZFsFdztIxERyV63ScHMHjGzN83s5U62m5mtMLM9ZlZnZlPStt1mZq+Fr9viDFxEROKXyZXCd4A5XWy/ErgwfM0H/g3AzM4BFgMfAaYBi81sSE+CFRFpddZZZwFw6NAh/vzP/7zDOpdddhndPeK+fPlyjhw5klqPOhR3Z/J1iO5uk4K7Pwe83UWVa4HvhmMyvQiUmtkIYDbwjLu/7e7vAM/QdXIREYls5MiRrFmzJuv92yeF/j4UdxxtCqOAg2nr9WFZZ+WnMLP5ZlZrZrWNjY0xhCQiOVW3Gr4xEe4rDb7Wre7R4WpqanjwwQdT663/Zb/33nvMmjWLKVOmMGnSJH70ox+dsu+BAweYOHEiAC0tLcybN4+Kigquu+66NkNn33XXXVRXVzNhwgQWL14MBIPsHTp0iMsvv5zLL78cODkUN8A///M/M3HiRCZOnMjy5ctT71fQQ3RnMpQqcD7wcifbfgz8adr6BqAa+BLwtbTye4EvdfdeGjpbpPdFGTrbd/zQ/e/PdV/8gZOvvz83KM/S1q1b/dJLL02tV1RU+G9/+1s/evSoNzc3u7t7Y2Ojf+hDH/ITJ064u/ugQYPcve2w2//0T//kd9xxRxDmjh1eVFTkmzdvdveTQ28fO3bMZ86c6Tt27HD3k0Nvt2pdr62t9YkTJ/p7773n7777ro8fP963bt2aiCG6kz50dgMwOm29LCzrrFxE8tmGJXC03X/GR1uC8ixVVVXx5ptvcujQIXbs2MGQIUMYPXo07s5XvvIVKisr+djHPkZDQwNvvPFGp8d57rnnuPXWW4Fg+OvKysrUttWrVzNlyhSqqqrYuXMnu3bt6jKm559/nuuuu45BgwZx1llncf3116cGzyvkIbrjSArrgL8Mn0KaDjS7++vAeuAKMxsSNjBfEZaJSD5rro9WnqEbbriBNWvW8MMf/pCbbroJgO9///s0NjayZcsWtm/fzrnnntvhkNnd2b9/Pw888AAbNmygrq6Oq6++OqvjtCrkIbozeST1ceAFYKyZ1ZvZJ83sTjO7M6zyNLAP2AP8O/AZAHd/G/g7YHP4WhKWiUg+G1wWrTxDN910E6tWrWLNmjXccMMNQPBf9gc/+EGKi4vZuHEjv/nNb7o8xqWXXsoPfvADAF5++WXq6uoA+P3vf8+gQYMYPHgwb7zxBj/5yU9S+3Q2bPcll1zC2rVrOXLkCH/4wx948sknueSSSyJ/X/k2RHe31yXufnM32x24u5NtjwCPZBeaiCTSrEXwH59rewupuCQo74EJEybw7rvvMmrUKEaMGAHALbfcwic+8QkmTZpEdXU148aN6/IYd911F3fccQcVFRVUVFQwdepUAC666CKqqqoYN24co0ePZsaMGal95s+fz5w5cxg5ciQbN25MlU+ZMoXbb7+dadOmAfCpT32KqqqqLm8VdebRRx/lzjvv5MiRI1xwwQV8+9vfTg3R3dzcjLunhui+99572bhxI6eddhoTJkzo9SG6C27obBGJLurQ2dStDtoQmuuDK4RZi6DyxtwFKG1o6GwRSZbKG5UECpTGPhIRkRQlBREBIGm3kqVjuf45KSmICAMHDuTw4cNKDAnn7hw+fJiBAwfm7D3UpiAilJWVUV9fj4aZSb6BAwdSVtazx3+7oqQgIhQXF1NeXt7XYUgC6PaRiIikKCmIiEiKkoKIiKQoKYiISIqSgoiIpCgpiIhIipKCiIikKCmIiEiKkoKIiKQoKYiISIqSgoiIpGSUFMxsjpntNrM9ZlbTwfYxZrbBzOrMbJOZlaVtO25m28PXujiDFxGReHU7IJ6ZFQEPAn8G1AObzWydu+9Kq/YA8F13f9TMPgrcD/zPcFuLu0+OOW4REcmBTK4UpgF73H2fu/8RWAVc267OeODZcHljB9tFRCQPZJIURgEH09brw7J0O4Drw+XrgLPNbGi4PtDMas3sRTOb29EbmNn8sE6txnMXEek7cTU0fwmYaWbbgJlAA3A83DbG3auBvwCWm9mH2u/s7ivdvdrdq4cPHx5TSCIiElUmk+w0AKPT1svCshR3P0R4pWBmZwH/w92bwm0N4dd9ZrYJqAL29jhyERGJXSZXCpuBC82s3MwGAPOANk8RmdkwM2s91kLgkbB8iJmd0VoHmAGkN1CLiEiCdJsU3P0YcA+wHngFWO3uO81siZldE1a7DNhtZr8GzgX+ISyvAGrNbAdBA/TSdk8tiYhIgpi793UMbVRXV3ttbW1fhyEiklfMbEvYftsjhdmjuW41fGMi3FcafK1b3dcRiYjkhUwamvNL3Wr4j8/B0ZZgvflgsA5QeWPfxSUikgcK70phw5KTCaHV0ZagXEREulR4SaG5Plq5iIikFF5SGFwWrVxERFIKLynMWgTFJW3LikuCchER6VLhNTS3NiZvWBLcMhpcFiSELhqZ125rYNn63RxqamFkaQkLZo9lblX74Z1ERApf4SUFCBJAhk8ard3WwMInXqLlaDBUU0NTCwufeAlAiUFE+p3Cu30U0bL1u1MJoVXL0eMsW7+7jyISEek7/T4pHGpqiVQuIlLI+n1SGFlaEqlcRKSQ9fuksGD2WEqKi9qUlRQXsWD22D6KSESk7xRmQ3MErY3JevpIRERJAQgSg5KAiIhuH4mISBolBRERSVFSEBGRFCUFERFJySgpmNkcM9ttZnvMrKaD7WPMbIOZ1ZnZJjMrS9t2m5m9Fr5uizN4ERGJV7dJwcyKgAeBK4HxwM1mNr5dtQeA77p7JbAEuD/c9xxgMfARYBqw2MyGxBe+iIjEKZMrhWnAHnff5+5/BFYB17arMx54NlzemLZ9NvCMu7/t7u8AzwBzeh52zDSns4gIkFk/hVHAwbT1eoL//NPtAK4H/gW4DjjbzIZ2su8pHQLMbD4wH+C8887LNPZ4RJzTWcNsi0ghi6uh+UvATDPbBswEGoDjXe9ykruvdPdqd68ePnx4TCFlKMKczq3DbDc0teCcHGZ77baG3olVRCTHMkkKDcDotPWysCzF3Q+5+/XuXgV8NSxrymTfPhdhTmcNsy0ihS6TpLAZuNDMys1sADAPWJdewcyGmVnrsRYCj4TL64ErzGxI2MB8RViWHBHmdNYw2yJS6LpNCu5+DLiH4MP8FWC1u+80syVmdk1Y7TJgt5n9GjgX+Idw37eBvyNILJuBJWFZckSY01nDbItIoTN37+sY2qiurvba2trefdO61RnN6dx+6k4Ihtm+//pJamwWkT5lZlvcvbqnx9EoqZDxnM4aZltECp2SQkQaZltECpnGPhIRkRQlBRERSVFSEBGRFCWFqDROkogUMDU0RxFxnCQRkXyjK4UoIoyTJCKSj5QUoogwTpKISD7S7aMoBpcFt4w6Ku+EhtoWkXyiK4UoIoyTBBpqW0Tyj5JCFJU3widWwODRgAVfP7Gi00ZmDbUtIvlGt4+iynCcJNBQ2yKSf5QUcmhkaQkNHSSArobaVhuEiPQlJYUcWjB7LM8/+RB/zSpG2lsc8mEsZx5/OvszHdZvPzR3axsEoMQgIr1CbQo5NLfoFywtfpiy097iNIOy095iafHDzC36RYf11QYhIn1NSSGXNizh9OPvtyk6/fj7nXZ2UxuEiPQ1JYVcitjZTdN9ikhfyygpmNkcM9ttZnvMrKaD7eeZ2UYz22ZmdWZ2VVh+vpm1mNn28PXNuL+BROusU1sn5Qtmj6WkuKhNWUlxEQtmj407MhGRDnWbFMysCHgQuBIYD9xsZuPbVfsasNrdq4B5wENp2/a6++TwdWdMceeHiJ3d5laN4v7rJzGqtAQDRpWWaP5nEelVmTx9NA3Y4+77AMxsFXAtsCutjgMfCJcHA4fiDDJvtfZn2LAkuGU0uCxICF30c9B0nyLSlzJJCqOA9AF/6oGPtKtzH/D/zOyzwCDgY2nbys1sG/B74Gvu/vP2b2Bm84H5AOedd17GweeFCJ3dgGB47ghJREQkTnE1NN8MfMfdy4CrgMfM7DTgdeC88LbSF4EfmNkH2u/s7ivdvdrdq4cPHx5TSHmodb6G5oOAn5yvQRP5iEgvySQpNACj09bLwrJ0nwRWA7j7C8BAYJi7/5e7Hw7LtwB7gQ/3NOiCpfkaRKSPZZIUNgMXmlm5mQ0gaEhe167Ob4FZAGZWQZAUGs1seNhQjZldAFwI7Isr+IKj+RpEpI91mxTc/RhwD7AeeIXgKaOdZrbEzK4Jq/0N8Gkz2wE8Dtzu7g5cCtSZ2XZgDXCnu7+di2+kIER8hFVEJG4WfHYnR3V1tdfW1vZ1GH2j/RzQEDzC2sXw3CIiAGa2xd2re3oc9WhOkojzNYiIxE2jpCZN1EdYRURipCsFERFJUVIQEZEUJQUREUlRm0Ke0/SdIhInJYU8puk7RSRuun2UxzR9p4jETUkhj2n6ThGJm5JCHtP0nSISNyWFPKbpO0UkbmpozmOtjcl6+khE4qKkkOfmFv2CuWcsgYH1cEYZFC0CNEyGiGRHSSGftR9VtXWmNtD4SSKSFbUp5DPN1CYiMVNSyGeaqU1EYqakkM80U5uIxCyjpGBmc8xst5ntMbOaDrafZ2YbzWybmdWZ2VVp2xaG++02s9lxBt/vzVoUzMyWrrgkKBcRyUK3ScHMioAHgSuB8cDNZja+XbWvEczdXAXMAx4K9x0frk8A5gAPhceTOGimNhGJWSZPH00D9rj7PgAzWwVcC+xKq+PAB8LlwcChcPlaYJW7/xew38z2hMd7IYbYBTRTm4jEKpPbR6OAg2nr9WFZuvuAW82sHnga+GyEfUVEJCHiami+GfiOu5cBVwGPmVnGxzaz+WZWa2a1jY2NMYUkIiJRZfLB3QCMTlsvC8vSfRJYDeDuLwADgWEZ7ou7r3T3anevHj58eObRi4hIrDJJCpuBC82s3MwGEDQcr2tX57fALAAzqyBICo1hvXlmdoaZlQMXAr+KK3jJQt1q+MZEuK80+Fq3uq8jEpEE6bah2d2Pmdk9wHqgCHjE3Xea2RKg1t3XAX8D/LuZfYGg0fl2d3dgp5mtJmiUPgbc7e7HO34nybm61Rz70Wc5/fj7wXrzwWAd1FgtIgBY8NmdHNXV1V5bW9vXYRSkI18fx5ktr59aXjKCM7/8ah9EJCJxMbMt7l7d0+OoR3M/MrDld5HKRaT/UVLoRw6dGBqpXET6HyWFfuThAbdyxAe0KTviA3h4wK2d76SGaZF+RUmhH5l89XwW+XzqTwzjhBv1J4axyOcz+er5He/QOl9D80HAT87XoMQgUrA0yU4/EkzT+RluWj8rs+k7u5qvQU8riRQkJYV+Zm7VqMzncNZ8DSL9jm4fSeeyma9BbRAieU1JQToXdb4GtUGI5D0lBelc5Y1snvS3/I7hnHDjdwxn86S/7bw9QXNGi+Q9tSlIp9Zua2Dh5jG0HP2XVFnJ5iLuH93QcbtENm0QdauDpNFcH9yWmrVIjdgifUhXCtKpZet303K07VBVLUePs2z97o53iNoGkc3tJrVZiOSUkoJ06lBTS6TyyG0QUW83qc1CJOeUFKRTI0tLIpVHnjM66u0mtVmI5JzaFKRTC2aPZeETL7W5hVRSXMSC2WM73ynKnNGDy8L/+jso74j6TYjknK4UpFNzq0Zx//WTGFVaggGjSku4//pJmXd+607U20290W9CbRbSz+lKQboUqQd0VK1XFJk+fTRrUdCGkH4LKZN+E631W9sg0t+7J/VFCpAm2ZFYrd3WwLL1uzMbWykbUR5h/cbETm5PjYYvvNzz+lHjEcmhuCbZ0ZWCxGbttoY2bRANTS0sfOIlgPgSQ5Q2i6htEFHLs7myUBIpXAXys82oTcHM5pjZbjPbY2Y1HWz/hpltD1+/NrOmtG3H07atizN4SZbI/RpyLWobRNTy3nikVm0i+aGAHpfuNimYWRHwIHAlMB642czGp9dx9y+4+2R3nwz8K/BE2uaW1m3ufk2MsUvCRO7XkGtRG7Kj1s/1I7VRP2iSlnSSlqBy+b1m87h00s5PKJMrhWnAHnff5+5/BFYB13ZR/2bg8TiCk/wSuV9DrkXtNxG1ftQri1wnkSQlnSQmqFwm2GxvPSbwyiKTpDAKSG99qw/LTmFmY4By4Nm04oFmVmtmL5rZ3E72mx/WqW1sbMwwdEmaBbPHUlJc1Kas234NuVZ5Y9BIfF9T8LW7e7xR6uf6kdpct4nkMukkKUFlE0/U+rm+9diL4u6nMA9Y4+7pN5bHhC3ifwEsN7MPtd/J3Ve6e7W7Vw8fPjzmkKS35LxfQ9JEvbLIdRJJUtJJUoLKJp6o5bm+9diLMnn6qAEYnbZeFpZ1ZB5wd3qBuzeEX/eZ2SagCtgbOVLJC1H7NeT8EdZci/I0VK77ZUStH7VHeZT6ue6tHrU8l98rRP/ZRj1+L8rkSmEzcKGZlZvZAIIP/lOeIjKzccAQ4IW0siFmdka4PAyYAeyKI3DJf62PsDY0teCcfIR17bbO/ucoAFFuT+W6TSSXDfFJuyrK9UMHkNtbj72o2ysFdz9mZvcA64Ei4BF332lmS4Bad29NEPOAVd62N1wF8C0zO0GQgJa6u5KCAF0/whrX1UK/uhKJWj/qf7dR6iftqiiX32s2cn38HlCPZukz5TVP0dFvnwH7l17d4T5RPuTbd6aDoOG7oNs58lnUzl8F0lksLurRLHlvZGkJDR30YejsEdaoPaZ740pEYpTLqyLJmEZJlT4T9RHWqD2mE9eZTiQPKClIn4n6CGvUD/nEdaYTyQO6fSR9KsojrFFvN2U1SZBIP6crBckbUW839bvOdCIx0JWC5I3WD/Moj5jmdJIgkQKkpCB5RR/yIrmlpCCSJmpnt7zvHCfSjpKCSChqP4hsZppTEpGkU0OzSChqP4io9fvlWE+Sd5QUREJR+0FELU/cdKUiHVBSEAlF7ewWtVw9rCUfKCmIhKL2g4haP4k9rNdua2DG0mcpr3mKGUuf1a0sUVIQaRW1s1vU+kmbrlRtHNIRDZ0t0ouS9MjrjKXPdjhsyKjSEn5R89FY3kN6j4bOFslDUTrfZfPIaxRq45CO6PaRSEJl87RSlDaCJLZxSN9TUhBJqKj/yUdtI0haG4ckQ0ZJwczmmNluM9tjZjUdbP+GmW0PX782s6a0bbeZ2Wvh67Y4gxcpZFH/k496ZaFRZKUj3bYpmFkR8CDwZ0A9sNnM1rn7rtY67v6FtPqfBarC5XOAxUA14MCWcN93Yv0uRApQ1PkgsmkjSNIAgxoCJBkyaWieBuxx930AZrYKuBbY1Un9mwkSAcBs4Bl3fzvc9xlgDvB4T4IW6Q+iDhUedRKibET54I5aN5eN6pK5TJLCKOBg2no98JGOKprZGKAceLaLfU/5CZvZfGA+wHnnnZdBSCL9Q5T/5HM901yUD+6oH/Jd3fpSUuhdcTc0zwPWuPvxbmumcfeV7l7t7tXDhw+POSSR/iHXbQRR2iyitm/o8djkyORKoQEYnbZeFpZ1ZB5wd7t9L2u376bMwxORKHLZRhDlgzvqh3zSbn31Z5lcKWwGLjSzcjMbQPDBv659JTMbBwwBXkgrXg9cYWZDzGwIcEVYJiJ5JsrTUFGfnMr147Ea0iNz3SYFdz8G3EPwYf4KsNrdd5rZEjO7Jq3qPGCVp42bETYw/x1BYtkMLGltdBaR/BLlgzvqh3ySbn216q+DBWrsIxHJWK6ePsp1LOU1T9HRJ50B+5de3eGxO2q0T3I/Do19JCK9LkqbRS7bN6I+3RS1zaI3noZKahuHhrkQkbwT9XZQ1NtZuX4aKsltHEoKIpJ3on5oR22zyPVggUmemlW3j0Qk72TzCGuSOgImuV+GrhREJO/k+hHWbJ6GKpRhy3WlICJ5J+q4UNm+R64mRMr1lUhPKCmISF5K0givUZ9W6o2kli0lBRGRHsr3YcvTqU1BRKSHktxGEJWSgohIDxXS1Ka6fSQi0kNJbiOISklBRCQGSW0jiEq3j0REJEVJQUREUpQUREQkRUlBRERSlBRERCRFSUFERFIySgpmNsfMdpvZHjOr6aTOjWa2y8x2mtkP0sqPm9n28LUursBFRCR+3fZTMLMi4EHgz4B6YLOZrXP3XWl1LgQWAjPc/R0z+2DaIVrcfXLMcYuISA5kcqUwDdjj7vvc/Y/AKuDadnU+DTzo7u8AuPub8YYpIiK9IZOkMAo4mLZeH5al+zDwYTP7hZm9aGZz0rYNNLPasHxuR29gZvPDOrWNjY2RvgEREYlPXMNcnA5cCFwGlAHPmdkkd28Cxrh7g5ldADxrZi+5+970nd19JbASoLq62mOKSUREIsrkSqEBGJ22XhaWpasH1rn7UXffD/yaIEng7g3h133AJqCqhzGLiEiOmHvX/5ib2ekEH/KzCJLBZuAv3H1nWp05wM3ufpuZDQO2AZOBE8ARd/+vsPwF4Nr0RuoO3q8R+E0nm4cBb2X6zfWBpMcHyY8x6fFB8mNMenyQ/BiTHh+cGuMYdx/e04N2e/vI3Y+Z2T3AeqAIeMTdd5rZEqDW3deF264ws13AcWCBux82s/8OfMvMThBclSztKiGE79fpN2Vmte5enfF318uSHh8kP8akxwfJjzHp8UHyY0x6fJC7GDNqU3D3p4Gn25UtSlt24IvhK73OfwKTeh6miIj0BvVoFhGRlHxLCiv7OoBuJD0+SH6MSY8Pkh9j0uOD5MeY9PggRzF229AsIiL9R75dKYiISA4pKYiISEpeJIVMRmnN0fuONrONaaO/fj4sP8fMnjGz18KvQ8JyM7MVYZx1ZjYl7Vi3hfU9izsFAAAFK0lEQVRfM7PbchBrkZltM7Mfh+vlZvbLMJYfmtmAsPyMcH1PuP38tGMsDMt3m9nsGGMrNbM1Zvaqmb1iZn+StHNoZl8If8Yvm9njZjawr8+hmT1iZm+a2ctpZbGdNzObamYvhfusMDOLIb5l4c+5zsyeNLPStG0dnpvO/r47O/89jTFt29+YmVvQhyox5zAs/2x4Hnea2T+mlef+HLp7ol8EfSP2AhcAA4AdwPheeu8RwJRw+WyCTnzjgX8EasLyGuDr4fJVwE8AA6YDvwzLzwH2hV+HhMtDYo71i8APgB+H66uBeeHyN4G7wuXPAN8Ml+cBPwyXx4fn9gygPDznRTHF9ijwqXB5AFCapHNIMJbXfqAk7dzd3tfnELgUmAK8nFYW23kDfhXWtXDfK2OI7wrg9HD562nxdXhu6OLvu7Pz39MYw/LRBP2rfgMMS9g5vBz4KXBGuP7B3jyHOftAjesF/AmwPm19IbCwj2L5EcEQ4ruBEWHZCGB3uPwtgp7drfV3h9tvBr6VVt6mXgxxlQEbgI8CPw5/Qd9K++NMncPwD+FPwuXTw3rW/rym1+thbIMJPnCtXXliziEnB308JzwnPwZmJ+EcAue3+8CI5byF215NK29TL9v42m27Dvh+uNzhuaGTv++ufofjiBFYA1wEHOBkUkjEOST4IP9YB/V65Rzmw+2jTEZpzbnwFkEV8EvgXHd/Pdz0O+DccLmzWHP9PSwH/hfBsCIAQ4Emdz/WwfulYgm3N4f1cxVjOdAIfNuC21sPm9kgEnQOPRif6wHgt8DrBOdkC8k5h+niOm+jwuVcxvpXBP89ZxNfV7/DPWJm1wIN7r6j3aaknMMPA5eEt31+ZmYXZxlfVucwH5JCnzOzs4D/C/y1u/8+fZsHKbjPnus1s48Db7r7lr6KoRunE1we/5u7VwF/ILjtkZKAcziEYI6QcmAkMAiY0+VOCdDX560rZvZV4Bjw/b6OJZ2ZnQl8BVjUXd0+dDrBVet0YAGwOmpbRU/kQ1LIZJTWnDGzYoKE8H13fyIsfsPMRoTbRwCtkwp1Fmsuv4cZwDVmdoBgAqSPAv8ClFowmGH790vFEm4fDBzOYYz1QL27/zJcX0OQJJJ0Dj8G7Hf3Rnc/CjxBcF6Tcg7TxXXeGsLl2GM1s9uBjwO3hIkrm/gO0/n574kPEST/HeHfTBmw1cz+WxYx5uoc1gNPeOBXBHcAhmURX3bnMJt7dL35Isia+wh+kK2NKBN66b0N+C6wvF35Mto29v1juHw1bRuqfhWWn0NwX31I+NoPnJODeC/jZEPz/6FtA9NnwuW7adtIujpcnkDbRqx9xNfQ/HNgbLh8X3j+EnMOgY8AO4Ezw/d9FPhsEs4hp95vju28cWoj6VUxxDcH2AUMb1evw3NDF3/fnZ3/nsbYbtsBTrYpJOUc3gksCZc/THBryHrrHMb6oZSrF8FTAb8maGH/ai++758SXJ7XAdvD11UE9+o2AK8RPCXQ+gtiBPNZ7wVeAqrTjvVXwJ7wdUeO4r2Mk0nhgvAXdk/4i9H6JMPAcH1PuP2CtP2/Gsa+m4hPUXQT12SgNjyPa8M/rESdQ+BvgVeBl4HHwj+8Pj2HwOMEbRxHCf57/GSc5w2oDr/fvcD/pt3DAFnGt4fgQ6z17+Wb3Z0bOvn77uz89zTGdtsPcDIpJOUcDgC+Fx53K/DR3jyHGuZCRERS8qFNQUREeomSgoiIpCgpiIhIipKCiIikKCmIiEiKkoKIiKQoKYiISMr/B2XUqG6DLRiHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(iterations, train_losses, label='train loss')\n",
    "plt.scatter(iterations, val_losses, label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reconstruct the outputs into vectors corresponding to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_predictions(preds, real_vectors):\n",
    "    preds_rec = []\n",
    "    cur_idx = 0\n",
    "    for i in real_vectors:\n",
    "        assembled = preds[cur_idx:cur_idx+len(i)]\n",
    "        preds_rec.append(assembled)\n",
    "        cur_idx += len(i)\n",
    "    return preds_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_preds_rec = reconstruct_predictions(validation_preds, validation_reals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the results with Q3 accuracy and segment of overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_acc(real, pred):\n",
    "    if real.shape[0] == 1:\n",
    "        real = np.squeeze(real, 0)\n",
    "    if pred.shape[0] == 1:\n",
    "        pred = np.squeeze(pred, 0)   \n",
    "    return np.sum(real==pred)/real.shape[0]\n",
    "\n",
    "\n",
    "def segment_of_overlap(ss_ref_str, ss_pred_str):\n",
    "    ss_ref = get_segments(ss_ref_str)\n",
    "    ss_pred = get_segments(ss_pred_str)\n",
    "    val_total = 0\n",
    "    N_total = 0\n",
    "    for k in ss_ref.keys():\n",
    "        subsum_k = 0\n",
    "        N_k = 0\n",
    "        for i in ss_ref[k]:\n",
    "            l_s1 = len(i[1])\n",
    "            N_k += l_s1\n",
    "            for j in ss_pred[k]:\n",
    "                l_s2 = len(j[1])\n",
    "                minov = len(np.intersect1d(i[1], j[1]))\n",
    "                if minov > 0:\n",
    "                    maxov = len(np.union1d(i[1], j[1]))\n",
    "                    delta = np.min([maxov-minov, minov, np.floor(l_s1/2), np.floor(l_s2/2)])\n",
    "                    value = l_s1*(minov+delta)/maxov\n",
    "                    subsum_k += value\n",
    "        N_total += N_k\n",
    "        val_total += subsum_k\n",
    "    sov = val_total/N_total\n",
    "    return sov\n",
    "\n",
    "def get_segments(ss_str):\n",
    "    ss_strs = [\"\".join(grp) for val, grp in itertools.groupby(ss_str)]\n",
    "    idx_lens = []\n",
    "    idx_start = 0\n",
    "    for i in ss_strs:\n",
    "        idx_lens.append(np.arange(idx_start, idx_start+len(i)))\n",
    "        idx_start += len(i)\n",
    "    segment_types = {'E':[], 'H':[], '-':[]}\n",
    "    for i in range(len(ss_strs)):\n",
    "        segment_types[ss_strs[i][0]].append([ss_strs[i], idx_lens[i]])\n",
    "    return segment_types\n",
    "\n",
    "def ints_to_symbols1d(ss_arr, map_dict=pos_ss_dict):\n",
    "    if len(ss_arr.shape) > 1:\n",
    "        for i,j in enumerate(ss_arr.shape):\n",
    "            if j == 1:\n",
    "                ss_arr = np.squeeze(ss_arr, i)\n",
    "    return \"\".join([map_dict[i] for i in ss_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6728813979986618"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracies = [q3_acc(i,j) for i,j in zip(validation_preds_rec, validation_reals)]\n",
    "np.mean(val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.concatenate([np.apply_along_axis(np.argmax, 1, net(i[0]).detach().numpy()) for i in testloader])\n",
    "test_reals = [tensorize_ss(test_items[i]['ss'], ss_id_dict, tensorize=False) for i in test_ids_filt]\n",
    "test_preds_rec = reconstruct_predictions(test_preds, test_reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6768144735083114"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracies_fcn =[q3_acc(i,j) for i,j in zip(test_preds_rec, test_reals)]\n",
    "np.mean(test_accuracies_fcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_psipred(prot_id, letter_to_number=ss_id_dict):\n",
    "    ss = ''\n",
    "    with open('/Users/Deathvoodoo/big_folders_docs/ss_pred/ss_predictions_psipred/{}.horiz'.format(prot_id)) as input:\n",
    "        lines = input.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            line = line.split()\n",
    "            if len(line)>0:\n",
    "                if line[0] == 'Pred:' and len(line)>1:\n",
    "                    ss += line[1]\n",
    "\n",
    "    ss = ss.replace('C', '-') # hyphens should be equivalent to coils\n",
    "    ss = np.array([letter_to_number[i] for i in ss])\n",
    "    return ss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "psipred_preds = [parse_psipred(i) for i in test_ids_filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[i.shape[0]/np.squeeze(j, 0).shape[0] for i,j in zip(psipred_preds, test_reals)] # sanity check for correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "psipred_acc = [q3_acc(i,j) for i,j in zip(psipred_preds, test_reals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7255188449781512"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(psipred_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check segment of overlap score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_str_fcn = [ints_to_symbols1d(i) for i in test_preds_rec]\n",
    "test_preds_str_psipred = [ints_to_symbols1d(i) for i in psipred_preds]\n",
    "test_reals_str = [ints_to_symbols1d(i) for i in test_reals] # same as grabbing strings directly from the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN SOV:  0.7256196222628652\n",
      "psipred SOV:  0.7850154998573716\n"
     ]
    }
   ],
   "source": [
    "test_sovs_fcn = [segment_of_overlap(i, j) for i,j in zip(test_preds_str_fcn, test_reals_str)]\n",
    "test_sovs_psipred = [segment_of_overlap(i, j) for i,j in zip(test_preds_str_psipred, test_reals_str)]\n",
    "\n",
    "print('CNN SOV: ', np.mean(test_sovs_fcn))\n",
    "print('psipred SOV: ', np.mean(test_sovs_psipred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bitdff059f72f8b417fb86b0d43a0194990"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
