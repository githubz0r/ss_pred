{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from http://www.compbio.dundee.ac.uk/jpred4/about_RETR_JNetv231_details.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(ss_str):\n",
    "    return [\"\".join(grp) for val, grp in itertools.groupby(ss_str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.loadtxt('train_names', dtype='str')\n",
    "test_ids = np.loadtxt('test_names', dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_items={}\n",
    "for i in train_ids:\n",
    "    with open('data/training/'+i+\".fasta\") as input:\n",
    "        seq = ''\n",
    "        lines = input.readlines()\n",
    "        for line in lines:\n",
    "            if line[0] != '>':\n",
    "                seq += line.strip()\n",
    "    with open('data/training/'+i+\".dssp\") as input:\n",
    "        ss = ''\n",
    "        lines = input.readlines()\n",
    "        for line in lines:\n",
    "            if line[0] != '>':\n",
    "                ss += line.strip()\n",
    "            \n",
    "    training_items[i] = {'seq':seq, 'ss':ss}\n",
    "    \n",
    "    \n",
    "test_items={}\n",
    "for i in test_ids:\n",
    "    with open('data/blind/'+i+\".fasta\") as input:\n",
    "        seq = ''\n",
    "        lines = input.readlines()\n",
    "        for line in lines:\n",
    "            if line[0] != '>':\n",
    "                seq += line.strip()\n",
    "    with open('data/blind/'+i+\".dssp\") as input:\n",
    "        ss = ''\n",
    "        lines = input.readlines()\n",
    "        for line in lines:\n",
    "            if line[0] != '>':\n",
    "                ss += line.strip()\n",
    "            \n",
    "    test_items[i] = {'seq':seq, 'ss':ss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_id_dict = {'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6, 'I': 7,\n",
    "              'K': 8, 'L': 9, 'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14, \n",
    "              'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19}\n",
    "\n",
    "pos_aa_dict = {j:i for i,j in aa_id_dict.items()}\n",
    "\n",
    "ss_id_dict = {'H':0, 'E':1, '-':2}\n",
    "\n",
    "def aa_to_onehot(aa_str, aa_to_nr=aa_id_dict, mask=None):\n",
    "    \"\"\"\n",
    "    Onehot encode an amino acid string using a letter to number dictionary.\n",
    "    The mask (from proteinnet files) is used to remove residues missing atoms from the primary sequence.\n",
    "    \"\"\"\n",
    "    if mask!=None:\n",
    "        mask_ind = np.asarray([x=='+' for x in mask])*1\n",
    "        mask_ind = np.nonzero(mask_ind)\n",
    "        aa_str = \"\".join([aa_str[x] for x in mask_ind[0]]) # the mask indices are a list in a list\n",
    "    init_array = np.zeros( (len(aa_to_nr.keys()), len(aa_str)) )\n",
    "    for i,j in enumerate(aa_str):\n",
    "        init_array[aa_to_nr[j], i] = 1\n",
    "    return init_array\n",
    "\n",
    "def label_to_index(ss, id_dict):\n",
    "    labels = np.array([id_dict[i] for i in ss])\n",
    "    return(labels)\n",
    "\n",
    "def onehot_to_str(onehot_arr):\n",
    "    '''Helper function to recover aa sequence from onehot encoding\n",
    "        input must be aa*N numpy array'''\n",
    "    aas = []\n",
    "    N = onehot_arr.shape[1]\n",
    "    for i in range(N):\n",
    "        pos = np.where(onehot_arr[:, i]>0)[0]\n",
    "        aas.append(pos_aa_dict[int(pos)])\n",
    "    return \"\".join(aas)\n",
    "\n",
    "def filter_proteins(prot_id, seq, allowed_symbols):\n",
    "    allowed = True\n",
    "    for i in seq:\n",
    "        if i not in allowed_symbols:\n",
    "            allowed = False\n",
    "    return allowed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds_filt = np.array([filter_proteins(i, training_items[i]['seq'], aa_id_dict.keys()) for i in train_ids])\n",
    "test_inds_filt = np.array([filter_proteins(i, test_items[i]['seq'], aa_id_dict.keys()) for i in test_ids])\n",
    "\n",
    "train_ids_filt = train_ids[train_inds_filt]\n",
    "test_ids_filt = test_ids[test_inds_filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1345,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids_filt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_ids_filt:\n",
    "    training_items[i]['seq_1h'] = aa_to_onehot(training_items[i]['seq'], aa_id_dict)[np.newaxis, :, :]\n",
    "    training_items[i]['ss_1h'] = aa_to_onehot(training_items[i]['ss'], ss_id_dict)[np.newaxis, :, :]\n",
    "    \n",
    "for i in test_ids_filt:\n",
    "    test_items[i]['seq_1h'] = aa_to_onehot(test_items[i]['seq'], aa_id_dict)[np.newaxis, :, :]\n",
    "    test_items[i]['ss_1h'] = aa_to_onehot(test_items[i]['ss'], ss_id_dict)[np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(678)\n",
    "inds_perm = np.random.permutation(len(train_ids_filt))\n",
    "val_prots = train_ids_filt[inds_perm[0:int(np.floor(len(inds_perm)*0.2))]]\n",
    "train_prots = train_ids_filt[inds_perm[int(np.floor(len(inds_perm)*0.2)):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class proteindataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, seqs, ss):\n",
    "        self.sequences = seqs\n",
    "        self.ss = ss\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.sequences[idx], self.ss[idx]]\n",
    "\n",
    "def protein_collate(batch):\n",
    "    seqs = [item[0] for item in batch]\n",
    "    ss = [item[1] for item in batch]\n",
    "    max_len = max([x.shape[2] for x in seqs])\n",
    "    for i in range(len(batch)):\n",
    "        curr_len = seqs[i].shape[2]\n",
    "        seq_padded = np.pad(seqs[i], ((0,0 ), (0,0), (0,max_len-curr_len)), constant_values = 0)\n",
    "        ss_padded = np.pad(ss[i], ((0,0 ), (0,0), (0,max_len-curr_len)), constant_values = 0)\n",
    "        seqs[i] = torch.tensor(seq_padded).float()\n",
    "        ss[i] = torch.tensor(ss_padded).float()\n",
    "    seq_tensor = torch.cat(seqs, 0)\n",
    "    ss_tensor = torch.cat(ss, 0)\n",
    "    return [seq_tensor, ss_tensor]\n",
    "\n",
    "def protein_collate2(batch):\n",
    "    seqs = [item[0] for item in batch]\n",
    "    ss = [item[1] for item in batch]\n",
    "    max_len = max([x.shape[2] for x in seqs])\n",
    "    for i in range(len(batch)):\n",
    "        curr_len = seqs[i].shape[2]\n",
    "        seq_padded = np.pad(seqs[i], ((0,0 ), (0,0), (0,max_len-curr_len)), constant_values = 0)\n",
    "        ss_padded = np.pad(ss[i], ((0,0 ), (0,max_len-curr_len)), constant_values = 0)\n",
    "        seqs[i] = torch.tensor(seq_padded).float()\n",
    "        ss[i] = torch.tensor(ss_padded).float()\n",
    "    seq_tensor = torch.cat(seqs, 0)\n",
    "    ss_tensor = torch.cat(ss, 0)\n",
    "    return [seq_tensor, ss_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = [training_items[i]['seq_1h'] for i in train_prots]\n",
    "train_ss = [training_items[i]['ss_1h'] for i in train_prots]\n",
    "\n",
    "val_seqs = [training_items[i]['seq_1h'] for i in val_prots]\n",
    "val_ss = [training_items[i]['ss_1h'] for i in val_prots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = proteindataset(train_seqs, train_ss)\n",
    "val_dataset = proteindataset(val_seqs, val_ss)\n",
    "test_dataset = proteindataset([test_items[i]['seq_1h'] for i in test_items.keys()], [test_items[i]['ss_1h'] for i in test_items.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 89)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ss[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2, collate_fn=protein_collate)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=4, collate_fn=protein_collate)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=4, collate_fn=protein_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "kd2 = 3\n",
    "pad2 = int((kd2-1)/2)\n",
    "kd3 = 5\n",
    "pad3 = int((kd3-1)/2)\n",
    "kd4 = 7\n",
    "pad4 = int((kd4-1)/2)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv0 = nn.Conv1d(20, 32, kernel_size=1)\n",
    "        self.conv0_bn = torch.nn.BatchNorm1d(32)\n",
    "        self.conv1 = nn.Conv1d(32, 32, kernel_size=kd2, padding=pad2) # down\n",
    "        self.conv1_bn = torch.nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=kd3, padding=pad3) # down \n",
    "        self.conv2_bn = torch.nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=kd4, padding=pad4) # down\n",
    "        self.conv3_bn = torch.nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=1)\n",
    "        self.conv4_bn = torch.nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose1d(in_channels=128, out_channels=64, kernel_size=kd4, padding=pad4) # up\n",
    "        self.deconv1_bn = torch.nn.BatchNorm1d(64)\n",
    "        self.conv5 = nn.Conv1d(64, 64, 1)\n",
    "        self.conv5_bn = torch.nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.deconv2 = nn.ConvTranspose1d(in_channels=128, out_channels=64, kernel_size=kd3, padding=pad3) # up\n",
    "        self.deconv2_bn = torch.nn.BatchNorm1d(64)\n",
    "        self.conv6 = nn.Conv1d(64, 64, 1)\n",
    "        self.conv6_bn = torch.nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.deconv3 = nn.ConvTranspose1d(in_channels=96, out_channels=32, kernel_size=kd2, padding=pad2) # up\n",
    "        self.deconv3_bn = torch.nn.BatchNorm1d(32)\n",
    "        self.conv7 = nn.Conv1d(32, 16, (1))\n",
    "        self.conv7_bn = torch.nn.BatchNorm1d(16)\n",
    "        self.conv8 = nn.Conv1d(16, 3, 1)\n",
    "        self.conv8_bn = torch.nn.BatchNorm1d(3)\n",
    "        self.conv9 = nn.Conv1d(3, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv0_out = F.relu(self.conv0_bn(self.conv0(x)))\n",
    "        conv1_out = F.relu(self.conv1_bn(self.conv1(conv0_out)))\n",
    "        conv2_out = F.relu(self.conv2_bn(self.conv2(conv1_out)))\n",
    "        conv3_out = F.relu(self.conv3_bn(self.conv3(conv2_out)))\n",
    "        conv4_out = F.relu(self.conv4_bn(self.conv4(conv3_out)))\n",
    "\n",
    "        deconv1_out = F.relu(self.deconv1_bn(self.deconv1(conv4_out)))\n",
    "        conv5_out = F.relu(self.conv5_bn(self.conv5(deconv1_out)))\n",
    "        \n",
    "        deconv2_input = torch.cat((conv2_out, deconv1_out), 1)  \n",
    "        deconv2_out = F.relu(self.deconv2_bn(self.deconv2(deconv2_input)))\n",
    "        conv6_out = F.relu(self.conv6_bn(self.conv6(deconv2_out)))\n",
    "        \n",
    "        deconv3_input = torch.cat((conv1_out, deconv2_out), 1)\n",
    "        deconv3_out = F.relu(self.deconv3_bn(self.deconv3(deconv3_input)))\n",
    "        conv7_out = F.relu(self.conv7_bn(self.conv7(deconv3_out)))\n",
    "        conv8_out = F.relu(self.conv8(conv7_out))\n",
    "        conv9_out = self.conv9(conv8_out)\n",
    "        return conv9_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 507]) torch.Size([1, 3, 507])\n",
      "torch.Size([1, 20, 95]) torch.Size([1, 3, 95])\n",
      "torch.Size([1, 20, 206]) torch.Size([1, 3, 206])\n",
      "torch.Size([1, 20, 70]) torch.Size([1, 3, 70])\n",
      "torch.Size([1, 20, 105]) torch.Size([1, 3, 105])\n",
      "torch.Size([1, 20, 52]) torch.Size([1, 3, 52])\n",
      "torch.Size([1, 20, 194]) torch.Size([1, 3, 194])\n",
      "torch.Size([1, 20, 206]) torch.Size([1, 3, 206])\n",
      "torch.Size([1, 20, 151]) torch.Size([1, 3, 151])\n",
      "torch.Size([1, 20, 114]) torch.Size([1, 3, 114])\n",
      "torch.Size([1, 20, 102]) torch.Size([1, 3, 102])\n",
      "torch.Size([1, 20, 83]) torch.Size([1, 3, 83])\n",
      "torch.Size([1, 20, 103]) torch.Size([1, 3, 103])\n",
      "torch.Size([1, 20, 153]) torch.Size([1, 3, 153])\n",
      "torch.Size([1, 20, 279]) torch.Size([1, 3, 279])\n",
      "torch.Size([1, 20, 95]) torch.Size([1, 3, 95])\n",
      "torch.Size([1, 20, 106]) torch.Size([1, 3, 106])\n",
      "torch.Size([1, 20, 46]) torch.Size([1, 3, 46])\n",
      "torch.Size([1, 20, 77]) torch.Size([1, 3, 77])\n",
      "torch.Size([1, 20, 124]) torch.Size([1, 3, 124])\n",
      "torch.Size([1, 20, 78]) torch.Size([1, 3, 78])\n",
      "torch.Size([1, 20, 469]) torch.Size([1, 3, 469])\n",
      "torch.Size([1, 20, 198]) torch.Size([1, 3, 198])\n",
      "torch.Size([1, 20, 220]) torch.Size([1, 3, 220])\n",
      "torch.Size([1, 20, 374]) torch.Size([1, 3, 374])\n",
      "torch.Size([1, 20, 145]) torch.Size([1, 3, 145])\n",
      "torch.Size([1, 20, 42]) torch.Size([1, 3, 42])\n",
      "torch.Size([1, 20, 105]) torch.Size([1, 3, 105])\n",
      "torch.Size([1, 20, 134]) torch.Size([1, 3, 134])\n",
      "torch.Size([1, 20, 137]) torch.Size([1, 3, 137])\n",
      "torch.Size([1, 20, 129]) torch.Size([1, 3, 129])\n",
      "torch.Size([1, 20, 336]) torch.Size([1, 3, 336])\n",
      "torch.Size([1, 20, 78]) torch.Size([1, 3, 78])\n",
      "torch.Size([1, 20, 116]) torch.Size([1, 3, 116])\n",
      "torch.Size([1, 20, 176]) torch.Size([1, 3, 176])\n",
      "torch.Size([1, 20, 95]) torch.Size([1, 3, 95])\n",
      "torch.Size([1, 20, 65]) torch.Size([1, 3, 65])\n",
      "torch.Size([1, 20, 81]) torch.Size([1, 3, 81])\n",
      "torch.Size([1, 20, 81]) torch.Size([1, 3, 81])\n",
      "torch.Size([1, 20, 121]) torch.Size([1, 3, 121])\n",
      "torch.Size([1, 20, 325]) torch.Size([1, 3, 325])\n",
      "torch.Size([1, 20, 66]) torch.Size([1, 3, 66])\n",
      "torch.Size([1, 20, 146]) torch.Size([1, 3, 146])\n",
      "torch.Size([1, 20, 202]) torch.Size([1, 3, 202])\n",
      "torch.Size([1, 20, 148]) torch.Size([1, 3, 148])\n",
      "torch.Size([1, 20, 101]) torch.Size([1, 3, 101])\n",
      "torch.Size([1, 20, 210]) torch.Size([1, 3, 210])\n",
      "torch.Size([1, 20, 69]) torch.Size([1, 3, 69])\n",
      "torch.Size([1, 20, 165]) torch.Size([1, 3, 165])\n",
      "torch.Size([1, 20, 157]) torch.Size([1, 3, 157])\n",
      "torch.Size([1, 20, 212]) torch.Size([1, 3, 212])\n",
      "torch.Size([1, 20, 56]) torch.Size([1, 3, 56])\n",
      "torch.Size([1, 20, 155]) torch.Size([1, 3, 155])\n",
      "torch.Size([1, 20, 117]) torch.Size([1, 3, 117])\n",
      "torch.Size([1, 20, 221]) torch.Size([1, 3, 221])\n",
      "torch.Size([1, 20, 102]) torch.Size([1, 3, 102])\n",
      "torch.Size([1, 20, 43]) torch.Size([1, 3, 43])\n",
      "torch.Size([1, 20, 100]) torch.Size([1, 3, 100])\n",
      "torch.Size([1, 20, 181]) torch.Size([1, 3, 181])\n",
      "torch.Size([1, 20, 43]) torch.Size([1, 3, 43])\n",
      "torch.Size([1, 20, 82]) torch.Size([1, 3, 82])\n",
      "torch.Size([1, 20, 128]) torch.Size([1, 3, 128])\n",
      "torch.Size([1, 20, 342]) torch.Size([1, 3, 342])\n",
      "torch.Size([1, 20, 97]) torch.Size([1, 3, 97])\n",
      "torch.Size([1, 20, 155]) torch.Size([1, 3, 155])\n",
      "torch.Size([1, 20, 162]) torch.Size([1, 3, 162])\n",
      "torch.Size([1, 20, 179]) torch.Size([1, 3, 179])\n",
      "torch.Size([1, 20, 162]) torch.Size([1, 3, 162])\n",
      "torch.Size([1, 20, 59]) torch.Size([1, 3, 59])\n",
      "torch.Size([1, 20, 84]) torch.Size([1, 3, 84])\n",
      "torch.Size([1, 20, 200]) torch.Size([1, 3, 200])\n",
      "torch.Size([1, 20, 333]) torch.Size([1, 3, 333])\n",
      "torch.Size([1, 20, 73]) torch.Size([1, 3, 73])\n",
      "torch.Size([1, 20, 209]) torch.Size([1, 3, 209])\n",
      "torch.Size([1, 20, 57]) torch.Size([1, 3, 57])\n",
      "torch.Size([1, 20, 288]) torch.Size([1, 3, 288])\n",
      "torch.Size([1, 20, 226]) torch.Size([1, 3, 226])\n",
      "torch.Size([1, 20, 82]) torch.Size([1, 3, 82])\n",
      "torch.Size([1, 20, 122]) torch.Size([1, 3, 122])\n",
      "torch.Size([1, 20, 99]) torch.Size([1, 3, 99])\n",
      "torch.Size([1, 20, 149]) torch.Size([1, 3, 149])\n",
      "torch.Size([1, 20, 96]) torch.Size([1, 3, 96])\n",
      "torch.Size([1, 20, 304]) torch.Size([1, 3, 304])\n",
      "torch.Size([1, 20, 80]) torch.Size([1, 3, 80])\n",
      "torch.Size([1, 20, 184]) torch.Size([1, 3, 184])\n",
      "torch.Size([1, 20, 122]) torch.Size([1, 3, 122])\n",
      "torch.Size([1, 20, 249]) torch.Size([1, 3, 249])\n",
      "torch.Size([1, 20, 31]) torch.Size([1, 3, 31])\n",
      "torch.Size([1, 20, 166]) torch.Size([1, 3, 166])\n",
      "torch.Size([1, 20, 113]) torch.Size([1, 3, 113])\n",
      "torch.Size([1, 20, 79]) torch.Size([1, 3, 79])\n",
      "torch.Size([1, 20, 62]) torch.Size([1, 3, 62])\n",
      "torch.Size([1, 20, 154]) torch.Size([1, 3, 154])\n",
      "torch.Size([1, 20, 109]) torch.Size([1, 3, 109])\n",
      "torch.Size([1, 20, 133]) torch.Size([1, 3, 133])\n",
      "torch.Size([1, 20, 157]) torch.Size([1, 3, 157])\n",
      "torch.Size([1, 20, 105]) torch.Size([1, 3, 105])\n",
      "torch.Size([1, 20, 148]) torch.Size([1, 3, 148])\n",
      "torch.Size([1, 20, 317]) torch.Size([1, 3, 317])\n",
      "torch.Size([1, 20, 71]) torch.Size([1, 3, 71])\n",
      "torch.Size([1, 20, 134]) torch.Size([1, 3, 134])\n",
      "torch.Size([1, 20, 60]) torch.Size([1, 3, 60])\n",
      "torch.Size([1, 20, 201]) torch.Size([1, 3, 201])\n",
      "torch.Size([1, 20, 132]) torch.Size([1, 3, 132])\n",
      "torch.Size([1, 20, 129]) torch.Size([1, 3, 129])\n",
      "torch.Size([1, 20, 395]) torch.Size([1, 3, 395])\n",
      "torch.Size([1, 20, 126]) torch.Size([1, 3, 126])\n",
      "torch.Size([1, 20, 474]) torch.Size([1, 3, 474])\n",
      "torch.Size([1, 20, 139]) torch.Size([1, 3, 139])\n",
      "torch.Size([1, 20, 214]) torch.Size([1, 3, 214])\n",
      "torch.Size([1, 20, 87]) torch.Size([1, 3, 87])\n",
      "torch.Size([1, 20, 404]) torch.Size([1, 3, 404])\n",
      "torch.Size([1, 20, 85]) torch.Size([1, 3, 85])\n",
      "torch.Size([1, 20, 176]) torch.Size([1, 3, 176])\n",
      "torch.Size([1, 20, 147]) torch.Size([1, 3, 147])\n",
      "torch.Size([1, 20, 152]) torch.Size([1, 3, 152])\n",
      "torch.Size([1, 20, 221]) torch.Size([1, 3, 221])\n",
      "torch.Size([1, 20, 122]) torch.Size([1, 3, 122])\n",
      "torch.Size([1, 20, 103]) torch.Size([1, 3, 103])\n",
      "torch.Size([1, 20, 52]) torch.Size([1, 3, 52])\n",
      "torch.Size([1, 20, 101]) torch.Size([1, 3, 101])\n",
      "torch.Size([1, 20, 240]) torch.Size([1, 3, 240])\n",
      "torch.Size([1, 20, 197]) torch.Size([1, 3, 197])\n",
      "torch.Size([1, 20, 146]) torch.Size([1, 3, 146])\n",
      "torch.Size([1, 20, 99]) torch.Size([1, 3, 99])\n",
      "torch.Size([1, 20, 286]) torch.Size([1, 3, 286])\n",
      "torch.Size([1, 20, 59]) torch.Size([1, 3, 59])\n",
      "torch.Size([1, 20, 36]) torch.Size([1, 3, 36])\n",
      "torch.Size([1, 20, 210]) torch.Size([1, 3, 210])\n",
      "torch.Size([1, 20, 126]) torch.Size([1, 3, 126])\n",
      "torch.Size([1, 20, 156]) torch.Size([1, 3, 156])\n",
      "torch.Size([1, 20, 93]) torch.Size([1, 3, 93])\n",
      "torch.Size([1, 20, 48]) torch.Size([1, 3, 48])\n",
      "torch.Size([1, 20, 187]) torch.Size([1, 3, 187])\n",
      "torch.Size([1, 20, 131]) torch.Size([1, 3, 131])\n",
      "torch.Size([1, 20, 213]) torch.Size([1, 3, 213])\n",
      "torch.Size([1, 20, 107]) torch.Size([1, 3, 107])\n",
      "torch.Size([1, 20, 116]) torch.Size([1, 3, 116])\n",
      "torch.Size([1, 20, 50]) torch.Size([1, 3, 50])\n",
      "torch.Size([1, 20, 362]) torch.Size([1, 3, 362])\n",
      "torch.Size([1, 20, 87]) torch.Size([1, 3, 87])\n",
      "torch.Size([1, 20, 143]) torch.Size([1, 3, 143])\n",
      "torch.Size([1, 20, 97]) torch.Size([1, 3, 97])\n",
      "torch.Size([1, 20, 82]) torch.Size([1, 3, 82])\n",
      "torch.Size([1, 20, 111]) torch.Size([1, 3, 111])\n",
      "torch.Size([1, 20, 146]) torch.Size([1, 3, 146])\n",
      "torch.Size([1, 20, 335]) torch.Size([1, 3, 335])\n",
      "torch.Size([1, 20, 216]) torch.Size([1, 3, 216])\n",
      "torch.Size([1, 20, 59]) torch.Size([1, 3, 59])\n",
      "torch.Size([1, 20, 83]) torch.Size([1, 3, 83])\n",
      "torch.Size([1, 20, 82]) torch.Size([1, 3, 82])\n",
      "torch.Size([1, 20, 79]) torch.Size([1, 3, 79])\n",
      "torch.Size([1, 20, 220]) torch.Size([1, 3, 220])\n",
      "torch.Size([1, 20, 177]) torch.Size([1, 3, 177])\n",
      "torch.Size([1, 20, 86]) torch.Size([1, 3, 86])\n",
      "torch.Size([1, 20, 262]) torch.Size([1, 3, 262])\n",
      "torch.Size([1, 20, 114]) torch.Size([1, 3, 114])\n",
      "torch.Size([1, 20, 95]) torch.Size([1, 3, 95])\n",
      "torch.Size([1, 20, 209]) torch.Size([1, 3, 209])\n",
      "torch.Size([1, 20, 202]) torch.Size([1, 3, 202])\n",
      "torch.Size([1, 20, 164]) torch.Size([1, 3, 164])\n",
      "torch.Size([1, 20, 82]) torch.Size([1, 3, 82])\n",
      "torch.Size([1, 20, 156]) torch.Size([1, 3, 156])\n",
      "torch.Size([1, 20, 118]) torch.Size([1, 3, 118])\n",
      "torch.Size([1, 20, 63]) torch.Size([1, 3, 63])\n",
      "torch.Size([1, 20, 35]) torch.Size([1, 3, 35])\n",
      "torch.Size([1, 20, 78]) torch.Size([1, 3, 78])\n",
      "torch.Size([1, 20, 274]) torch.Size([1, 3, 274])\n",
      "torch.Size([1, 20, 158]) torch.Size([1, 3, 158])\n",
      "torch.Size([1, 20, 151]) torch.Size([1, 3, 151])\n",
      "torch.Size([1, 20, 98]) torch.Size([1, 3, 98])\n",
      "torch.Size([1, 20, 145]) torch.Size([1, 3, 145])\n",
      "torch.Size([1, 20, 220]) torch.Size([1, 3, 220])\n",
      "torch.Size([1, 20, 282]) torch.Size([1, 3, 282])\n",
      "torch.Size([1, 20, 80]) torch.Size([1, 3, 80])\n",
      "torch.Size([1, 20, 235]) torch.Size([1, 3, 235])\n",
      "torch.Size([1, 20, 120]) torch.Size([1, 3, 120])\n",
      "torch.Size([1, 20, 118]) torch.Size([1, 3, 118])\n",
      "torch.Size([1, 20, 324]) torch.Size([1, 3, 324])\n",
      "torch.Size([1, 20, 89]) torch.Size([1, 3, 89])\n",
      "torch.Size([1, 20, 264]) torch.Size([1, 3, 264])\n",
      "torch.Size([1, 20, 178]) torch.Size([1, 3, 178])\n",
      "torch.Size([1, 20, 275]) torch.Size([1, 3, 275])\n",
      "torch.Size([1, 20, 197]) torch.Size([1, 3, 197])\n",
      "torch.Size([1, 20, 77]) torch.Size([1, 3, 77])\n",
      "torch.Size([1, 20, 74]) torch.Size([1, 3, 74])\n",
      "torch.Size([1, 20, 184]) torch.Size([1, 3, 184])\n",
      "torch.Size([1, 20, 81]) torch.Size([1, 3, 81])\n",
      "torch.Size([1, 20, 131]) torch.Size([1, 3, 131])\n",
      "torch.Size([1, 20, 110]) torch.Size([1, 3, 110])\n",
      "torch.Size([1, 20, 355]) torch.Size([1, 3, 355])\n",
      "torch.Size([1, 20, 113]) torch.Size([1, 3, 113])\n",
      "torch.Size([1, 20, 143]) torch.Size([1, 3, 143])\n",
      "torch.Size([1, 20, 328]) torch.Size([1, 3, 328])\n",
      "torch.Size([1, 20, 82]) torch.Size([1, 3, 82])\n",
      "torch.Size([1, 20, 161]) torch.Size([1, 3, 161])\n",
      "torch.Size([1, 20, 123]) torch.Size([1, 3, 123])\n",
      "torch.Size([1, 20, 129]) torch.Size([1, 3, 129])\n",
      "torch.Size([1, 20, 33]) torch.Size([1, 3, 33])\n",
      "torch.Size([1, 20, 80]) torch.Size([1, 3, 80])\n",
      "torch.Size([1, 20, 158]) torch.Size([1, 3, 158])\n",
      "torch.Size([1, 20, 65]) torch.Size([1, 3, 65])\n",
      "torch.Size([1, 20, 79]) torch.Size([1, 3, 79])\n",
      "torch.Size([1, 20, 312]) torch.Size([1, 3, 312])\n",
      "torch.Size([1, 20, 146]) torch.Size([1, 3, 146])\n",
      "torch.Size([1, 20, 740]) torch.Size([1, 3, 740])\n",
      "torch.Size([1, 20, 138]) torch.Size([1, 3, 138])\n",
      "torch.Size([1, 20, 59]) torch.Size([1, 3, 59])\n",
      "torch.Size([1, 20, 77]) torch.Size([1, 3, 77])\n",
      "torch.Size([1, 20, 382]) torch.Size([1, 3, 382])\n",
      "torch.Size([1, 20, 117]) torch.Size([1, 3, 117])\n",
      "torch.Size([1, 20, 483]) torch.Size([1, 3, 483])\n",
      "torch.Size([1, 20, 114]) torch.Size([1, 3, 114])\n",
      "torch.Size([1, 20, 158]) torch.Size([1, 3, 158])\n",
      "torch.Size([1, 20, 87]) torch.Size([1, 3, 87])\n",
      "torch.Size([1, 20, 85]) torch.Size([1, 3, 85])\n",
      "torch.Size([1, 20, 145]) torch.Size([1, 3, 145])\n",
      "torch.Size([1, 20, 97]) torch.Size([1, 3, 97])\n",
      "torch.Size([1, 20, 128]) torch.Size([1, 3, 128])\n",
      "torch.Size([1, 20, 522]) torch.Size([1, 3, 522])\n",
      "torch.Size([1, 20, 302]) torch.Size([1, 3, 302])\n",
      "torch.Size([1, 20, 124]) torch.Size([1, 3, 124])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 102]) torch.Size([1, 3, 102])\n",
      "torch.Size([1, 20, 81]) torch.Size([1, 3, 81])\n",
      "torch.Size([1, 20, 111]) torch.Size([1, 3, 111])\n",
      "torch.Size([1, 20, 118]) torch.Size([1, 3, 118])\n",
      "torch.Size([1, 20, 143]) torch.Size([1, 3, 143])\n",
      "torch.Size([1, 20, 104]) torch.Size([1, 3, 104])\n",
      "torch.Size([1, 20, 105]) torch.Size([1, 3, 105])\n",
      "torch.Size([1, 20, 241]) torch.Size([1, 3, 241])\n",
      "torch.Size([1, 20, 135]) torch.Size([1, 3, 135])\n",
      "torch.Size([1, 20, 95]) torch.Size([1, 3, 95])\n",
      "torch.Size([1, 20, 124]) torch.Size([1, 3, 124])\n",
      "torch.Size([1, 20, 358]) torch.Size([1, 3, 358])\n",
      "torch.Size([1, 20, 101]) torch.Size([1, 3, 101])\n",
      "torch.Size([1, 20, 142]) torch.Size([1, 3, 142])\n",
      "torch.Size([1, 20, 161]) torch.Size([1, 3, 161])\n",
      "torch.Size([1, 20, 82]) torch.Size([1, 3, 82])\n",
      "torch.Size([1, 20, 147]) torch.Size([1, 3, 147])\n",
      "torch.Size([1, 20, 47]) torch.Size([1, 3, 47])\n",
      "torch.Size([1, 20, 131]) torch.Size([1, 3, 131])\n",
      "torch.Size([1, 20, 264]) torch.Size([1, 3, 264])\n",
      "torch.Size([1, 20, 117]) torch.Size([1, 3, 117])\n",
      "torch.Size([1, 20, 85]) torch.Size([1, 3, 85])\n",
      "torch.Size([1, 20, 106]) torch.Size([1, 3, 106])\n",
      "torch.Size([1, 20, 115]) torch.Size([1, 3, 115])\n",
      "torch.Size([1, 20, 84]) torch.Size([1, 3, 84])\n",
      "torch.Size([1, 20, 118]) torch.Size([1, 3, 118])\n",
      "torch.Size([1, 20, 46]) torch.Size([1, 3, 46])\n",
      "torch.Size([1, 20, 138]) torch.Size([1, 3, 138])\n",
      "torch.Size([1, 20, 90]) torch.Size([1, 3, 90])\n",
      "torch.Size([1, 20, 60]) torch.Size([1, 3, 60])\n",
      "torch.Size([1, 20, 88]) torch.Size([1, 3, 88])\n",
      "torch.Size([1, 20, 260]) torch.Size([1, 3, 260])\n",
      "torch.Size([1, 20, 88]) torch.Size([1, 3, 88])\n",
      "torch.Size([1, 20, 130]) torch.Size([1, 3, 130])\n",
      "torch.Size([1, 20, 285]) torch.Size([1, 3, 285])\n",
      "torch.Size([1, 20, 124]) torch.Size([1, 3, 124])\n",
      "torch.Size([1, 20, 59]) torch.Size([1, 3, 59])\n",
      "torch.Size([1, 20, 112]) torch.Size([1, 3, 112])\n",
      "torch.Size([1, 20, 142]) torch.Size([1, 3, 142])\n",
      "torch.Size([1, 20, 52]) torch.Size([1, 3, 52])\n",
      "torch.Size([1, 20, 144]) torch.Size([1, 3, 144])\n",
      "torch.Size([1, 20, 140]) torch.Size([1, 3, 140])\n",
      "torch.Size([1, 20, 298]) torch.Size([1, 3, 298])\n",
      "torch.Size([1, 20, 190]) torch.Size([1, 3, 190])\n",
      "torch.Size([1, 20, 142]) torch.Size([1, 3, 142])\n",
      "torch.Size([1, 20, 127]) torch.Size([1, 3, 127])\n",
      "torch.Size([1, 20, 39]) torch.Size([1, 3, 39])\n"
     ]
    }
   ],
   "source": [
    "for i in valloader:\n",
    "    a,b = i\n",
    "    print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 37] training loss: 0.7913225857835067, validation_loss: 0.7409491118002118\n",
      "epoch: 0, iteration: 75] training loss: 0.7764715473902853, validation_loss: 0.7141156296304612\n",
      "new best validation loss, saving..\n",
      "epoch: 0, iteration: 113] training loss: 0.7647296221632707, validation_loss: 0.7035960793273598\n",
      "new best validation loss, saving..\n",
      "epoch: 0, iteration: 151] training loss: 0.7576816960384971, validation_loss: 0.6991304802850276\n",
      "new best validation loss, saving..\n",
      "epoch: 0, iteration: 189] training loss: 0.75337815284729, validation_loss: 0.69147964370295\n",
      "new best validation loss, saving..\n",
      "epoch: 0, iteration: 227] training loss: 0.7425126198091005, validation_loss: 0.6829944531048984\n",
      "new best validation loss, saving..\n",
      "epoch: 0, iteration: 265] training loss: 0.7478521889761874, validation_loss: 0.6645979687405346\n",
      "new best validation loss, saving..\n",
      "epoch: 1, iteration: 37] training loss: 0.7449629808727064, validation_loss: 0.6578628306716789\n",
      "new best validation loss, saving..\n",
      "epoch: 1, iteration: 75] training loss: 0.725997363266192, validation_loss: 0.6657839510520598\n",
      "epoch: 1, iteration: 113] training loss: 0.7163447659266623, validation_loss: 0.6462102598860363\n",
      "new best validation loss, saving..\n",
      "epoch: 1, iteration: 151] training loss: 0.715849696021331, validation_loss: 0.6510021756351221\n",
      "epoch: 1, iteration: 189] training loss: 0.7114606970234921, validation_loss: 0.6453145074356886\n",
      "new best validation loss, saving..\n",
      "epoch: 1, iteration: 227] training loss: 0.7154106058572468, validation_loss: 0.6612299853541151\n",
      "epoch: 1, iteration: 265] training loss: 0.7068635382150349, validation_loss: 0.6652738710096779\n",
      "epoch: 2, iteration: 37] training loss: 0.6999920449758831, validation_loss: 0.6499369771507151\n",
      "epoch: 2, iteration: 75] training loss: 0.696867502049396, validation_loss: 0.6431962548578536\n",
      "new best validation loss, saving..\n",
      "epoch: 2, iteration: 113] training loss: 0.6866485884315089, validation_loss: 0.6405448643248319\n",
      "new best validation loss, saving..\n",
      "epoch: 2, iteration: 151] training loss: 0.6936873966141751, validation_loss: 0.6404546016210966\n",
      "new best validation loss, saving..\n",
      "epoch: 2, iteration: 189] training loss: 0.6946140763006712, validation_loss: 0.6425680954660179\n",
      "epoch: 2, iteration: 227] training loss: 0.7036363011912296, validation_loss: 0.641982741737011\n",
      "epoch: 2, iteration: 265] training loss: 0.7022654178895449, validation_loss: 0.6509100978702422\n",
      "epoch: 3, iteration: 37] training loss: 0.6909231527855522, validation_loss: 0.6451731769568857\n",
      "epoch: 3, iteration: 75] training loss: 0.698844784184506, validation_loss: 0.6365934081901851\n",
      "new best validation loss, saving..\n",
      "epoch: 3, iteration: 113] training loss: 0.6879025446741205, validation_loss: 0.6584909280215046\n",
      "epoch: 3, iteration: 151] training loss: 0.6922940376557802, validation_loss: 0.6437020452492295\n",
      "epoch: 3, iteration: 189] training loss: 0.6896760369602003, validation_loss: 0.635360430164408\n",
      "new best validation loss, saving..\n",
      "epoch: 3, iteration: 227] training loss: 0.7001238653534337, validation_loss: 0.6445566830138734\n",
      "epoch: 3, iteration: 265] training loss: 0.6776920823674453, validation_loss: 0.6428025495607167\n",
      "epoch: 4, iteration: 37] training loss: 0.6874102699129205, validation_loss: 0.639625651796511\n",
      "epoch: 4, iteration: 75] training loss: 0.6759112147908461, validation_loss: 0.6513206286279688\n",
      "epoch: 4, iteration: 113] training loss: 0.6875337221120533, validation_loss: 0.6385846277595003\n",
      "epoch: 4, iteration: 151] training loss: 0.6861544204385657, validation_loss: 0.6356647065581007\n",
      "epoch: 4, iteration: 189] training loss: 0.6981068677023837, validation_loss: 0.6520070790358197\n",
      "epoch: 4, iteration: 227] training loss: 0.6835420915954992, validation_loss: 0.6329315773394915\n",
      "new best validation loss, saving..\n",
      "epoch: 4, iteration: 265] training loss: 0.6952064398087954, validation_loss: 0.6378941512683957\n",
      "epoch: 5, iteration: 37] training loss: 0.6871175436597121, validation_loss: 0.6338244672158397\n",
      "epoch: 5, iteration: 75] training loss: 0.6781594768950814, validation_loss: 0.6389121793238206\n",
      "epoch: 5, iteration: 113] training loss: 0.6852181726380399, validation_loss: 0.6349864027092444\n",
      "epoch: 5, iteration: 151] training loss: 0.672538045205568, validation_loss: 0.644611740090147\n",
      "epoch: 5, iteration: 189] training loss: 0.6922040236623663, validation_loss: 0.63625945743575\n",
      "epoch: 5, iteration: 227] training loss: 0.6839903498950758, validation_loss: 0.6341686667562857\n",
      "epoch: 5, iteration: 265] training loss: 0.6880171032328355, validation_loss: 0.636918037465071\n",
      "epoch: 6, iteration: 37] training loss: 0.6717645353392551, validation_loss: 0.6360879026381062\n",
      "epoch: 6, iteration: 75] training loss: 0.6738689635929308, validation_loss: 0.6483414158723608\n",
      "epoch: 6, iteration: 113] training loss: 0.6931677303816143, validation_loss: 0.635769190176712\n",
      "epoch: 6, iteration: 151] training loss: 0.6893594625749087, validation_loss: 0.6399216413719503\n",
      "epoch: 6, iteration: 189] training loss: 0.673532004419126, validation_loss: 0.6352447599283376\n",
      "epoch: 6, iteration: 227] training loss: 0.6684380157997734, validation_loss: 0.6367994006238461\n",
      "epoch: 6, iteration: 265] training loss: 0.6716412125449431, validation_loss: 0.6343474798042975\n",
      "epoch: 7, iteration: 37] training loss: 0.6826435829463758, validation_loss: 0.6482302088941334\n",
      "epoch: 7, iteration: 75] training loss: 0.6936206864683252, validation_loss: 0.6408019340614404\n",
      "epoch: 7, iteration: 113] training loss: 0.6848163604736328, validation_loss: 0.6324556569627672\n",
      "new best validation loss, saving..\n",
      "epoch: 7, iteration: 151] training loss: 0.6901153733855799, validation_loss: 0.6350883527980866\n",
      "epoch: 7, iteration: 189] training loss: 0.6653959186453569, validation_loss: 0.6385070475503861\n",
      "epoch: 7, iteration: 227] training loss: 0.6699852708138918, validation_loss: 0.6500389174900978\n",
      "epoch: 7, iteration: 265] training loss: 0.6730468006510484, validation_loss: 0.6374332027364398\n",
      "epoch: 8, iteration: 37] training loss: 0.668630766241174, validation_loss: 0.6347147148781112\n",
      "epoch: 8, iteration: 75] training loss: 0.6637319326400757, validation_loss: 0.6436708328005992\n",
      "epoch: 8, iteration: 113] training loss: 0.6724408692435214, validation_loss: 0.6370428456028154\n",
      "epoch: 8, iteration: 151] training loss: 0.6685346364974976, validation_loss: 0.6312371222059964\n",
      "new best validation loss, saving..\n",
      "epoch: 8, iteration: 189] training loss: 0.6781752438921678, validation_loss: 0.6365727912980832\n",
      "epoch: 8, iteration: 227] training loss: 0.6750061402195379, validation_loss: 0.6354340972953568\n",
      "epoch: 8, iteration: 265] training loss: 0.6637760510570124, validation_loss: 0.6349843946531357\n",
      "epoch: 9, iteration: 37] training loss: 0.6754920451264632, validation_loss: 0.6346994153185846\n",
      "epoch: 9, iteration: 75] training loss: 0.6610996785916781, validation_loss: 0.6338363673164056\n",
      "epoch: 9, iteration: 113] training loss: 0.680804663582852, validation_loss: 0.6474100647141053\n",
      "epoch: 9, iteration: 151] training loss: 0.6728816283376593, validation_loss: 0.6337171215771743\n",
      "epoch: 9, iteration: 189] training loss: 0.667969098216609, validation_loss: 0.64316309550881\n",
      "epoch: 9, iteration: 227] training loss: 0.668936445524818, validation_loss: 0.6384072859934272\n",
      "epoch: 9, iteration: 265] training loss: 0.6632423793014727, validation_loss: 0.6352234011910662\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "class BinaryDiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss of binary class\n",
    "    Args:\n",
    "        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n",
    "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
    "        predict: A tensor of shape [N, *]\n",
    "        target: A tensor of shape same with predict\n",
    "        reduction: Reduction method to apply, return mean over batch if 'mean',\n",
    "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
    "    Returns:\n",
    "        Loss tensor according to arg reduction\n",
    "    Raise:\n",
    "        Exception if unexpected reduction\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1, p=2, reduction='mean'):\n",
    "        super(BinaryDiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n",
    "        predict = predict.contiguous().view(predict.shape[0], -1)\n",
    "        target = target.contiguous().view(target.shape[0], -1)\n",
    "\n",
    "        num = torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n",
    "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n",
    "\n",
    "        loss = 1 - num / den\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return loss\n",
    "        else:\n",
    "            raise Exception('Unexpected reduction {}'.format(self.reduction))\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss, need one hot encode input\n",
    "    Args:\n",
    "        weight: An array of shape [num_classes,]\n",
    "        ignore_index: class index to ignore\n",
    "        predict: A tensor of shape [N, C, *]\n",
    "        target: A tensor of same shape with predict\n",
    "        other args pass to BinaryDiceLoss\n",
    "    Return:\n",
    "        same as BinaryDiceLoss\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, ignore_index=None, **kwargs):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.kwargs = kwargs\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        #print(predict.shape, target.shape)\n",
    "        assert predict.shape == target.shape, 'predict & target shape do not match'\n",
    "        dice = BinaryDiceLoss(**self.kwargs)\n",
    "        total_loss = 0\n",
    "        predict = F.softmax(predict, dim=1)\n",
    "\n",
    "        for i in range(target.shape[1]):\n",
    "            if i != self.ignore_index:\n",
    "                dice_loss = dice(predict[:, i], target[:, i])\n",
    "                if self.weight is not None:\n",
    "                    assert self.weight.shape[0] == target.shape[1], \\\n",
    "                        'Expect weight shape [{}], get[{}]'.format(target.shape[1], self.weight.shape[0])\n",
    "                    dice_loss *= self.weights[i]\n",
    "                total_loss += dice_loss\n",
    "\n",
    "        return total_loss/target.shape[1]\n",
    "\n",
    "criterion = DiceLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "prints_per_epoch = 7\n",
    "\n",
    "verbose_k = np.floor(len(trainloader)/prints_per_epoch)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "iterations = []\n",
    "best_loss = None\n",
    "patience_val = 5\n",
    "patience_counter = patience_val\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        sequence, true_angles = data\n",
    "        #print(sequence.shape, true_angles.shape)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        predicted_angles = net(sequence)\n",
    "\n",
    "        loss = criterion(predicted_angles, true_angles)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # adding to running loss, we will output this at every verbose_k\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (i+1) % verbose_k == 0:\n",
    "            if patience_counter < 1:\n",
    "                break\n",
    "            train_losses.append(running_loss/verbose_k)\n",
    "            true_iter = len(trainloader)*epoch + i\n",
    "            iterations.append(true_iter)\n",
    "            net.eval()\n",
    "            validation_loss = 0\n",
    "            for j in valloader:\n",
    "                pred_k = net(j[0])\n",
    "                loss_k = criterion(pred_k, j[1]).item()\n",
    "                validation_loss += loss_k/len(val_seqs)\n",
    "            val_losses.append(validation_loss)\n",
    "            net.train()\n",
    "            print('epoch: {}, iteration: {}] training loss: {}, validation_loss: {}'.format(\n",
    "                epoch, i, running_loss/verbose_k, validation_loss))\n",
    "\n",
    "            if best_loss == None:\n",
    "                best_loss = validation_loss\n",
    "            else:\n",
    "                if validation_loss <= min(val_losses):\n",
    "                    patience_counter = patience_val\n",
    "                    print('new best validation loss, saving..')\n",
    "                    best_loss = validation_loss\n",
    "                    torch.save(net.state_dict(), 'best_fcn_parameters.pt')\n",
    "                else:\n",
    "                    patience_counter -= 1\n",
    "            \n",
    "\n",
    "            running_loss = 0.0\n",
    "            \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('/Users/Deathvoodoo/big_folders_docs/ss_pred/best_fcn_parameters.pt'))\n",
    "\n",
    "net.eval()\n",
    "\n",
    "validation_preds = [np.apply_along_axis(np.argmax, 1, net(i[0]).detach().numpy()) for i in valloader]\n",
    "#validation_reals = [np.apply_along_axis(np.argmax, 1, i[1].detach().numpy()) for i in valloader]\n",
    "validation_reals = [np.apply_along_axis(np.argmax, 1, i) for i in val_ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(iterations, train_losses, label='train loss')\n",
    "plt.scatter(iterations, val_losses, label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_acc(real, pred):\n",
    "    if real.shape[0] == 1:\n",
    "        real = np.squeeze(real, 0)\n",
    "    if pred.shape[0] == 1:\n",
    "        pred = np.squeeze(pred, 0)   \n",
    "    return np.sum(real==pred)/real.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7003473571625122"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracies = [q3_acc(i,j) for i,j in zip(validation_preds, validation_reals)]\n",
    "np.mean(val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = [np.apply_along_axis(np.argmax, 1, net(i[0]).detach().numpy()) for i in testloader]\n",
    "test_reals = [np.apply_along_axis(np.argmax, 1, i[1].detach().numpy()) for i in testloader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7042728643247672"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracies_fcn = [q3_acc(i,j) for i,j in zip(test_preds, test_reals)]\n",
    "np.mean(test_accuracies_fcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_psipred(prot_id, letter_to_number=ss_id_dict):\n",
    "    ss = ''\n",
    "    with open('/Users/Deathvoodoo/big_folders_docs/ss_pred/ss_predictions_psipred/{}.horiz'.format(prot_id)) as input:\n",
    "        lines = input.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            line = line.split()\n",
    "            if len(line)>0:\n",
    "                if line[0] == 'Pred:' and len(line)>1:\n",
    "                    ss += line[1]\n",
    "\n",
    "    ss = ss.replace('C', '-') # hyphens should be equivalent to coils\n",
    "    ss = np.array([letter_to_number[i] for i in ss])\n",
    "    return ss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "psipred_preds = [parse_psipred(i) for i in test_items.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[i.shape[0]/np.squeeze(j, 0).shape[0] for i,j in zip(psipred_preds, test_reals)] # sanity check for correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "psipred_acc = [q3_acc(i,j) for i,j in zip(psipred_preds, test_reals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7255188449781512"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(psipred_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bitdff059f72f8b417fb86b0d43a0194990"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
